{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"os.chdir('../input')\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check here for the distribution of classes among your dataset. This leads a way to creation of an unbiased model as the classes with lesser proportion can be treated specially **"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train['label'].unique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['label'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.countplot(train['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check for any outliers in the data using the describe function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check for any missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Now since all the EDA for the data is done and no issues have been found we proceed to *Data Preprocessing***"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(labels = ['label'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = train['label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The code below takes care of scaling of the data. This brings the range of the column values from (0,255) to (0,1)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train/255\ntest = test/255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The code below converts the target variable to its categorical form i.e a vector of a definite size so as to fit the output layer of the neural netwok**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = to_categorical(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(Y_train,axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now in order to use CNN, the tabular data format has to be converted to image format that is a pixel matrix. The 784 data points will be converted to a 28x28 matrix. Since the images are grayscale images the third dimension will be 1 , hence the final matrix will be of the form 28x28x1. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_matrix = X_train.values.reshape(-1,28,28,1)\ntest_matrix = test.values.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now since the values are reshaped, we split the training dataset here into what is called the train and validation dataset using the sklearn.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_val,Y_train,Y_val = train_test_split(X_train_matrix,Y_train,test_size  = 0.1,random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now since the input values have been converted as such that they can be used with CNNs, hence now we will start creating the architecture of the model i.e the layers of the model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.core import Flatten\nfrom keras.layers import Conv2D,Dense,Dropout,MaxPool2D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(filters=32,padding='Same',kernel_size=(5,5),activation='relu',input_shape=(28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters=32,padding='Same',kernel_size=(5,5),activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Conv2D(filters=64,kernel_size=(2,2),padding='Same',activation='relu'))\nmodel.add(Conv2D(filters=64,kernel_size=(2,2),padding='Same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(filters=64,kernel_size=(2,2),activation='relu',padding='Same'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(10,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = RMSprop(lr=0.001,rho=0.9,epsilon=1e-08,decay=0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc',patience = 5,verbose = 1,factor = 0.3,min_lr = 0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch = 22\nbatch_size = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(featurewise_center=False,\n                            featurewise_std_normalization=False,\n                            samplewise_center=False,\n                            samplewise_std_normalization=False,\n                            zca_whitening=False,\n                            rotation_range=30,\n                            zoom_range=0.1,\n                            horizontal_flip=False,\n                            vertical_flip=False,\n                            width_shift_range=0.1,\n                            height_shift_range=0.1)\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(datagen.flow(X_train,Y_train,batch_size=batch_size),\n                             epochs=epoch,verbose=1,steps_per_epoch=X_train.shape[0]//batch_size,\n                             validation_data = (X_val,Y_val),callbacks=[learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluate the model**\n\n\nNow since the model has been trained and has achieved a good validation accuracy, one needs to evaluate the model in order to check for model performance.\nSome complex models and mainly neural networks have a tendency to overfit on the training data due to complex calculations carried out on the data during training.\nOverfitting is a curse for predictive models as they learn everything about training data but fail to perform on unseen data. Hence the concept of validation set is applied which is a dataset which is compleely unseen by the model and is only used to be evaluated on at the end of each epoch.\nIf the training accuracy and the validation accuracy coincide then the model is not overfit and if the accuracy is high it ready to predict on the test dataset. \nIf the training accuracy is high and the valiodation accuracy is low then it is a case of overfitting.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'],color='b',label='Training Loss')\nax[0].plot(history.history['val_loss'],color='r',label='Validation Loss',axes=ax[0])\nlegend = ax[0].legend(loc='best',shadow=True)\nax[1].plot(history.history['acc'],color='b',label='Accuracy')\nax[1].plot(history.history['val_acc'],color='r',label='Valoidation Accuracy')\nlegen = ax[1].legend(loc='best',shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly this is not a case of overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nY_pred = model.predict(X_val)\nY_pred = np.argmax(Y_pred,axis=1)\nY_true = np.argmax(Y_val,axis=1)\ncm = confusion_matrix(Y_pred,Y_true)\nplt.imshow(cm,interpolation='nearest',cmap=plt.cm.Blues)\nplt.title('Confusion_matrix')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.predict(test_matrix)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"../input/submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}